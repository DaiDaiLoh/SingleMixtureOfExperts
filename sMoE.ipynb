{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Build a loader to get the data</h1>\n",
    "<h3>Note that we use the \"Schedule Free\" Adam from Facebook research: https://github.com/facebookresearch/schedule_free<br/>\n",
    "This relieves us from the annoying task of providing a learning rate schedule for our transformer (i.e. adapt the learning rate in some pattern throughout training);<br/>\n",
    "However, it still needs warmup.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import time\n",
    "import schedulefree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#some global variables to make our life easier\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #move model to GPU if available\n",
    "STEPS_WARMUP        = 2500 #warmup steps for the model\n",
    "STEPS_WARMUP_ROUTER =  500 #warmup steps for the router\n",
    "STEPS_ROUTER        = STEPS_WARMUP_ROUTER * 2 #how long do we train the router at all; after that, we just use the most likely cluster (=better for training)\n",
    "BATCH_SIZE = 64+16\n",
    "TEXTFILES_TO_USE = 1 #####max: 128, then the whole \"tiny stories\" dataset is used; 8/128 data chunks is a good value for trying stuff out\n",
    "NO_CLUSTERS = 10 #number of clusters for our single model mixture of experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack_\\AppData\\Local\\Temp\\ipykernel_3192\\2821808655.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rules = torch.load(\"BPE/rules.pt\")\n",
      "C:\\Users\\Jack_\\AppData\\Local\\Temp\\ipykernel_3192\\2821808655.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  index_to_char = torch.load(\"BPE/index_to_char.pt\")\n",
      "C:\\Users\\Jack_\\AppData\\Local\\Temp\\ipykernel_3192\\2821808655.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  char_to_index = torch.load(\"BPE/char_to_index.pt\")\n"
     ]
    }
   ],
   "source": [
    "rules = torch.load(\"BPE/rules.pt\")\n",
    "index_to_char = torch.load(\"BPE/index_to_char.pt\")\n",
    "char_to_index = torch.load(\"BPE/char_to_index.pt\")\n",
    "\n",
    "PADDING_TOKEN = len(index_to_char) #padding token is the last token in the index_to_char dictionary; we use it to shorter pad sequences to the max length\n",
    "LARGEST_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#helper functions - the same as for our BPE\n",
    "def apply_BPE(text, rules):\n",
    "    text_as_indices = transcribe_chars_to_index(text)\n",
    "    for rule in rules:\n",
    "        i = 0\n",
    "        while i < len(text_as_indices)-1:\n",
    "            if text_as_indices[i] == rule[0][0] and text_as_indices[i+1] == rule[0][1]:\n",
    "                text_as_indices[i] = rule[1]\n",
    "                text_as_indices.pop(i+1)\n",
    "            i += 1\n",
    "            \n",
    "    return text_as_indices\n",
    "\n",
    "def transcribe_indices_to_chars(indices):\n",
    "    return [index_to_char[indices[i]] for i in range(0, len(indices))]\n",
    "\n",
    "def decode_BPE(tokens):\n",
    "    #EXPECTS input to be ONE item, not a batch!\n",
    "    #cut off every token >= PADDING_TOKEN!\n",
    "    tokens_ = tokens\n",
    "    tokens = []\n",
    "    for token in tokens_:\n",
    "        if token < PADDING_TOKEN:\n",
    "            #if token is a tensor, append item:\n",
    "            if isinstance(token, torch.Tensor):\n",
    "                tokens.append(token.item())\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "        else:\n",
    "            break\n",
    "    #return \"\".join(transcribe_indices_to_chars(tokens)) #show full text\n",
    "    return str(transcribe_indices_to_chars(tokens)) #show individual text fragments, in an array\n",
    "\n",
    "def transcribe_chars_to_index(chars):\n",
    "    indices = []\n",
    "    for char in chars:\n",
    "        indices.append(char_to_index[char])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load from multiple files, pad to a fixed length / filter out longer ones</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack_\\AppData\\Local\\Temp\\ipykernel_3192\\960099049.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  132269  samples from  8  files.\n",
      "Stored  107065  tensors.\n",
      "Loaded  21953  samples from  1  files.\n",
      "Stored  18280  tensors.\n"
     ]
    }
   ],
   "source": [
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.data = []\n",
    "        for file_path in file_paths:\n",
    "            loaded = torch.load(file_path)\n",
    "            self.data.extend(loaded)\n",
    "        print(\"Loaded \", len(self.data), \" samples from \", len(file_paths), \" files.\")\n",
    "        \n",
    "        if False: #print some statsitics; usually, throwing away samples with 250+ doesn't hurt much, but speeds up computation considerably\n",
    "            largest = 0 #find the largest sequence length\n",
    "            largest_sequence = None\n",
    "            lengths = []\n",
    "            for sample in self.data:\n",
    "                lengths.append(len(sample))\n",
    "                if len(sample) > largest:\n",
    "                    largest = len(sample)\n",
    "                    largest_sequence = sample\n",
    "            \n",
    "            print(\"Average sequence length: \", sum(lengths)/len(lengths))\n",
    "            print(\"Median sequence length: \", sorted(lengths)[len(lengths)//2])\n",
    "            print(\"90 percent length: \", sorted(lengths)[int(len(lengths)/10*8)])\n",
    "\n",
    "            print(\"Largest sequence length: \", largest)\n",
    "            print(\"Example of largest sequence: \", decode_BPE(largest_sequence))\n",
    "        \n",
    "        #throw away samples with LARGEST_SEQUENCE_LENGTH+ tokens\n",
    "        #   -> without this, attention computation is very slow (quadratic scaling) with little benefit (very few samples are actually that long)\n",
    "        self.data = [torch.cat((sample, torch.ones((LARGEST_SEQUENCE_LENGTH-len(sample),), dtype=torch.long) * PADDING_TOKEN)) for sample in self.data if len(sample) < LARGEST_SEQUENCE_LENGTH]\n",
    "        #stack into one big tensor; for tinystories, we can fit everything into memory at the same time!\n",
    "        self.data = torch.stack(self.data)\n",
    "        print(\"Stored \", len(self.data), \" tensors.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample\n",
    "\n",
    "#list all textfiles we want to load\n",
    "textfiles = []\n",
    "for i in range(0, TEXTFILES_TO_USE):\n",
    "    textfiles.append(\"data/train_BPE_\"+str(i)+\".dat\")\n",
    "\n",
    "train_dataset = EncodedDataset(textfiles)\n",
    "test_dataset = EncodedDataset([\"data/validation_BPE.dat\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Transformer helpers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embed_dims, SEQUENCE_LENGTH):\n",
    "    enc = torch.arange(SEQUENCE_LENGTH).unsqueeze(1).float()  # Use arange instead of ones\n",
    "    denominator = torch.pow(10000, torch.arange(0, embed_dims, 2).float() / embed_dims)\n",
    "    \n",
    "    angle_rads = enc / denominator\n",
    "    sin_vals = torch.sin(angle_rads)\n",
    "    cos_vals = torch.cos(angle_rads)\n",
    "    \n",
    "    pos_enc = torch.zeros(SEQUENCE_LENGTH, embed_dims)\n",
    "    pos_enc[:, 0::2] = sin_vals\n",
    "    pos_enc[:, 1::2] = cos_vals\n",
    "    return pos_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Build actual Transformer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rough rundown of what a transformer does:</h3>\n",
    "https://arxiv.org/abs/1706.03762 is the original idea; https://jalammar.github.io/illustrated-transformer/ explains it somewhat nicely:<br/>\n",
    "For attention, you compute pairwise scores between all tokens, then use these scores to<br/>\n",
    "mix your tokens together to new tokens. Exemplary, for \"A black cat sat on the wall\", the word \"black\" will \"attend\" to \"cat\", i.e. have a lot of attention on black;<br/>\n",
    "meaning the tokens will be mixed such that we have a hybrid token thingy that says \"black cat\" (very crude).<br/><br/>\n",
    "Transformers are just build out of stacked blocks (\"layers\"); each block consists of:<br/>\n",
    "-an attention layer that computes pairwise scores, then re-mixes tokens accordingly<br/>\n",
    "-normalisations & residuals<br/>\n",
    "-a fully connected network part that is applied to EACH token after attention; meaning this does the heavy lifting,<br/>\n",
    "while the attention is the only part where tokens get to know each other. The fully connected network is also the part where mixture of experts (MoE)<br/>\n",
    "usually happens, i.e. where we apply a different network according to some routing process (\"for math, we use net A, for french, net D, for german, net F, [...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Our MoE idea:</h3>\n",
    "<b><u>Observation A:</u></b><br/>\n",
    "Wisdom of the crowd effects benefit most tasks; If you ask 1000 people to estimate something, the average will come pretty close;<br/>\n",
    " an ensemble ofestimators will usually be better than a single one<br/>\n",
    "<b><u>Observation B:</u></b><br/>\n",
    "Applying dropout (randomly disabling some neurons) essentially brings such an ensemble effect: the network learns to work with many different combinations of neurons<br/>\n",
    "<b><u>Observation C:</u></b><br/>\n",
    "Mixture of Experts is, in a way, a selective ensemble, but fails to use knowledge that everyone has; general knowledge has to be in each expert (e.g. how grammar works, even if it's the history node or the math node, they need to individually learn grammar)<br/><br/>\n",
    "We bring these observations together in ONE idea:\n",
    "<h1>Single Mixture-of-Experts</h1>\n",
    "We apply a simple routing to determine which expert gets asked; However, instead of physically different netowrks that, individually, don't know any information from the other ones, <br/>\n",
    "we instead use dropout to determine an expert: For a given transformer, we apply an additional, deterministic dropout procedure. This dropout (\"which neurons are turned off\") are determined<br/>\n",
    "by the router, i.e. are content-dependant.<br/>\n",
    "In result, we don't have multiple experts, but a mixture of experts <b>in a single network</b>, as a result of selectively dropping out neurons according to which \"expert\", we'd want.<br/>\n",
    "We still get to keep \"one\" network to not lose any knowledge when branching into an expert, but still allow some degree of specialisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code blocks here:</h3>\n",
    "<b>AutoregressiveDecoderTransformer</b> is a decoder-only transformer (i.e. predicts away token after token); it stacks multiple transformer decoder blocks,<br/>\n",
    "then applies some head that does classification (i.e. gives us a pseudo probability distributuon over tokens) that we sample from.<br/><br/>\n",
    "\n",
    "<b>TransformerDecoderBlock</b> is a transformer block: attention computation between all tokens, then apply the fully connected network to each token. Also contains normalisation and residuals<br/><br/>\n",
    "\n",
    "<b>FeedForward</b> is the fully connected network that processes each token after \"mixing\" it in the attention layer<br/><br/>\n",
    "\n",
    "<b>CausalSelfAttention</b> is there to speed up training: When we process a full sentence, we can just \"cover up\" some part of the attention matrix to get a subset that describes part of a sentence;<br/>\n",
    "if we have \"The| |black| |cat\", we can compute the whole 5-by-5 attention matrix (whole sentence) and just cover up pieces of it for e.g. \"The| |black\". This is what makes transformers so fast -<br/>\n",
    "they somewhat learn on each prefix in parallel.<br/><br/>\n",
    "\n",
    "<b>LinearPlusDropout</b> is a regular linear layer, but can also receive a class label; if it receives a class label, it performs a deterministic dropout.<br/>\n",
    "Meaning: If we give it e.g. label \"5\" multiple times, it will always zero out the same neurons, but a different dropout than if we give it the label \"3\"<br/><br/>\n",
    "\n",
    "<b>AutoregressiveRouterTransformer</b> is our router; also stacks transformer blocks, but then proceeds to output a label to which \"expert\" we branch out for the current part of the sentence (=which dropout we perform)<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "#set to 0.0 for no dropout\n",
    "DROPOUT_RATE = 0.1\n",
    "sMoE_DROPOUT_RATE = 0.2 #make it large enough so each expert is significantly different from the others\n",
    "\n",
    "class LinearPlusDropout(nn.Module):\n",
    "    #our special way of applying dropout is built in here:\n",
    "    #a regular linear layer, but with a dropout mask that is applied to the output\n",
    "    #   (i.e. we don't just zero out some weights, but a deterministic set of neurons)\n",
    "    def __init__(self, in_features, out_features, no_classes=NO_CLUSTERS):\n",
    "        super(LinearPlusDropout, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "        self.out = out_features\n",
    "\n",
    "        #make mask a parameter:\n",
    "        self.mask = nn.Parameter((torch.rand(no_classes, self.out) > sMoE_DROPOUT_RATE).float(), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x, expert_labels=None):\n",
    "        x = self.linear(x)\n",
    "        #should be [b]-sized\n",
    "        if expert_labels != None:\n",
    "            shape_b4 = x.size()\n",
    "            #flatten to [-1, out_features]\n",
    "            expert_labels = expert_labels.view(-1)\n",
    "            x = x.view(-1, x.size()[-1])\n",
    "            #find individual mask for each class\n",
    "            mask = self.mask[expert_labels]\n",
    "            x = x * mask\n",
    "\n",
    "            scale = mask.sum(dim=-1) / self.out\n",
    "            x = x / scale[:,None]\n",
    "            #reshape back to original shape\n",
    "            x = x.view(shape_b4)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    #feed forward with ReLU; is applied to each token individually\n",
    "    #   (i.e. we don't care about the order of tokens here, just apply the same operation to each token)\n",
    "    #   this is basically where the knowledge is at\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        #two linear layers with ReLU in between\n",
    "        #note how size expands and contracts:\n",
    "        #   imagine a puzzle you try to solve - you want the table you do it on\n",
    "        #   to be big enough to lay out all the pieces instead of just big enough \n",
    "        #   to hold the final result\n",
    "        self.lin_1 = LinearPlusDropout(dim, dim * 4)\n",
    "        self.lin_2 = LinearPlusDropout(dim * 4, dim)\n",
    "        #I'd suggest leaky ReLU, but ReLU is the standard used in TFs\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, expert_labels=None):\n",
    "        x = self.relu(self.lin_1(x, expert_labels))\n",
    "        x = self.lin_2(x, expert_labels)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    #>>Causal<< self-attention means that the model can only look at previous tokens:\n",
    "    #   this is important for autoregressive models, as we can then recycle a lot\n",
    "    #   of the computation for attention & train all prefixes at once\n",
    "    #   (e.g. for ABCDEFG, we train the next token after A, after AB, after ABC, ... in one go)\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        #ensure that we can split the dimension into num_heads\n",
    "        #each head will then have dim/num_heads dimensions, i.e.\n",
    "        #we divide the input into num_heads parts to \n",
    "        #   a) keep matrix sizes tame\n",
    "        #   b) ensure that different heads can focus on different tasks\n",
    "        #      (softmax focuses the attention largely on one part of the input,\n",
    "        #       then different heads can focus on different parts)\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        #scale factor for attention - as the dot product grows with the dimension, \n",
    "        #   we scale it down to prevent the softmax from getting too extreme / sharp\n",
    "        self.scale = 1.0 / (math.sqrt(self.head_dim))\n",
    "\n",
    "        #linear layer for query, key, value (=apply query, key, value matrix to input)\n",
    "        #here, we make life easy for us and apply one linear layer (=also multiplies the input by a matrix)\n",
    "        #   (instead of having separate matrices for each)\n",
    "        #   --> same number of parameters, but less code / all in one go\n",
    "        #   (don't forget the bias=False, as we don't want to add a bias here; we just want a matrix multiplication, essentially)\n",
    "        #   (linear layer is just W * x + b)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(dim, 3 * dim, bias=False)\n",
    "        \n",
    "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, T, C = x.shape\n",
    "        #produce key, query, value from input\n",
    "        qkv = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        #divide into num_heads parts (=split the dimension up)\n",
    "        q, k, v = map(lambda t: t.view(batch_size, T, self.num_heads, self.head_dim).transpose(1, 2), qkv)\n",
    "\n",
    "        #compute scaled dot product attention:\n",
    "        #   dot product of query and key, then scale it down to prevent softmax from getting too extreme\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        #to prevent the model from looking at future tokens (for autoregressive training),\n",
    "        #   we mask the attention weights for tokens that are in the future;\n",
    "        #   this is done by setting the attention weights for future tokens to -inf\n",
    "        #   (as softmax(-inf) = 0, i.e. the model will ignore these tokens)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        out = (attn_weights @ v).transpose(1, 2).contiguous().view(batch_size, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    #transformer decoder block:\n",
    "    #   self attention, dropout\n",
    "    #   residual & layer norm\n",
    "    #   feed forward, dropout\n",
    "    #   residual & layer norm\n",
    "\n",
    "    def __init__(self, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(dim, num_heads)\n",
    "        self.ffn = FeedForward(dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        #dropout just to prevent overfitting, i.e. memorising stuff:\n",
    "        #   we want the model to learn the structure of the data, not the data itself!\n",
    "        #   dropout is a simple way to prevent the model from memorising the data\n",
    "        #   by randomly setting some weights to zero;\n",
    "        #   i.e. the model can't rely on just memorising individual aspects,\n",
    "        #   but has to learn the structure of the data in a general and redundant (=robust) way\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None, expert_labels=None):\n",
    "        #self attention and dropout, then residual\n",
    "        x = x + self.dropout(self.attn(x, mask))\n",
    "        #layer norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        #feed forward and dropout, then residual\n",
    "        x = x + self.dropout(self.ffn(x, expert_labels))\n",
    "        #layer norm\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class AutoregressiveDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, dim, num_layers, num_heads, dropout=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        #embed tokens with something learnable\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
    "        #embed positions with something computed\n",
    "        self.pos_embedding = torch.nn.Parameter(positional_encoding(dim, LARGEST_SEQUENCE_LENGTH+1)[None], requires_grad=False)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_seq_len, max_seq_len)).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x, expert_labels=None):\n",
    "        batch_size, tokens = x.shape\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_emb = self.pos_embedding[:,:x.size()[1]]\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        mask = self.mask[:, :, :tokens, :tokens]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask, expert_labels)\n",
    "\n",
    "        return self.decoder(x)\n",
    "\n",
    "    #maximum sampling - useful for debugging, but will always generate the same sequence (\"always pick the most likely token as next token\")\n",
    "    def generate_max(self, tokens, max_new_tokens, router):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices = router(tokens).argmax(dim=-1)\n",
    "            logits = self.forward(tokens, indices)\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=False)[:, -1:]\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        return tokens\n",
    "    #sample just randomly according to probability - has a chance to pick some really messed up\n",
    "    def generate_mul(self, tokens, max_new_tokens, router):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices = router(tokens).argmax(dim=-1)\n",
    "            logits = self.forward(tokens, indices)\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1)[:, -1], 1)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        return tokens\n",
    "    def generate_nuc(self, tokens, max_new_tokens, router):\n",
    "        for _ in range(max_new_tokens):\n",
    "            #1. get logits\n",
    "            indices = router(tokens).argmax(dim=-1)\n",
    "            logits = self.forward(tokens, indices)\n",
    "            #2. turn into probabilities\n",
    "            probs = F.softmax(logits, dim=-1)[:, -1]\n",
    "            #3. sort & cumsum to get the cumulative probability to cut off everything beyond 90%\n",
    "            sorted, indices = torch.sort(probs, descending=True)\n",
    "            cumulative = torch.cumsum(sorted, dim=-1)\n",
    "            #find the first index where the cumulative probability is larger than 0.9\n",
    "            cutoff = torch.argmax((cumulative > 0.9).long(), dim=-1)\n",
    "            #4. null out everything beyond 90%\n",
    "            for b in range(0, probs.size()[0]):\n",
    "                cutoff_index = cutoff[b] + 1\n",
    "                probs[b, indices[b, cutoff_index:]] = 0.0\n",
    "            #5. sample from the modified probabilities\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        return tokens\n",
    "    \n",
    "class AutoregressiveRouterTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size_in, max_seq_len, dim, num_layers, num_heads, dropout=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        #embed tokens with something learnable\n",
    "        self.token_embedding = nn.Embedding(vocab_size_in, dim)\n",
    "        #embed positions with something computed\n",
    "        self.pos_embedding = torch.nn.Parameter(positional_encoding(dim, LARGEST_SEQUENCE_LENGTH+1)[None], requires_grad=False)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder = nn.Linear(dim, NO_CLUSTERS, bias=False)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_seq_len, max_seq_len)).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, tokens = x.shape\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_emb = self.pos_embedding[:,:x.size()[1]]\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        mask = self.mask[:, :, :tokens, :tokens]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>\n",
    "We always train the router ALONGSIDE the full transformer<br/>\n",
    "Rough idea: Router produces a probability distribution from which we sample;<br/>\n",
    "We then find the tokens that produces higher-than-average error; Those are the ones our router<br/>\n",
    "should assign a new probability to. We do so by reducing the probability of those chosen tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has  27634176  parameters.\n",
      "Router has  7003648  parameters.\n",
      "Losses Tokens:  5.009021067619324\n",
      "Router  Correctness:  0.3907765462994576\n",
      "Router Distribution:  23.067965030670166\n",
      "Router Distribution LOCAL:  33.40509033203125\n",
      "Index distribution:  tensor([2005, 2016, 1763, 1704, 1842, 2372, 2024, 2045, 2078, 2151],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([1739, 1548,  652,  405, 1128, 4824, 2448, 1853, 2815, 2668],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1470167.6250, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  8.732993670359049  minutes; Currently,  73  steps in.\n",
      "Losses Tokens:  4.7302128076553345\n",
      "Router  Correctness:  0.30810931921005247\n",
      "Router Distribution:  15.431269145011902\n",
      "Router Distribution LOCAL:  10.69532585144043\n",
      "Index distribution:  tensor([1927, 1968, 1991, 1910, 2160, 2019, 2026, 2116, 2052, 1831],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([1475, 1463, 1906, 1383, 3553, 1466, 1406, 2916, 3490, 1022],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(801896., device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  8.098089029901736  minutes; Currently,  148  steps in.\n",
      "Losses Tokens:  4.661130452156067\n",
      "Router  Correctness:  0.2924159958958626\n",
      "Router Distribution:  9.573974424600602\n",
      "Router Distribution LOCAL:  14.147371292114258\n",
      "Index distribution:  tensor([1887, 2035, 1940, 2083, 2130, 2039, 2044, 1938, 2131, 1773],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([ 522, 2261,  879, 2827, 3743, 3146, 2483, 1175, 2723,  321],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1280022.3750, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  7.513470911021743  minutes; Currently,  224  steps in.\n",
      "Losses Tokens:  4.502095794677734\n",
      "Router  Correctness:  0.27764273062348366\n",
      "Router Distribution:  11.364441895484925\n",
      "Router Distribution LOCAL:  12.44698429107666\n",
      "Index distribution:  tensor([1999, 2030, 2153, 2017, 2102, 1857, 2094, 1927, 1822, 1999],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([ 853, 1895, 6502, 2504, 3284,  305, 1627,  844,  277, 1989],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(3081465., device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  6.969869788302316  minutes; Currently,  300  steps in.\n",
      "Losses Tokens:  4.23244321346283\n",
      "Router  Correctness:  0.2708925731480122\n",
      "Router Distribution:  8.750773891806602\n",
      "Router Distribution LOCAL:  13.537714958190918\n",
      "Index distribution:  tensor([1799, 1990, 2112, 2082, 1886, 2008, 2167, 1909, 2018, 2029],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([ 255,  934, 4789, 2609,  489, 1663, 5310,  398, 1938, 1695],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(2834684.7500, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  6.4447599844412595  minutes; Currently,  376  steps in.\n",
      "Losses Tokens:  3.7096805453300474\n",
      "Router  Correctness:  0.2788695454597473\n",
      "Router Distribution:  8.678442478179932\n",
      "Router Distribution LOCAL:  6.770515441894531\n",
      "Index distribution:  tensor([1979, 2183, 1966, 1893, 2001, 1963, 2051, 1995, 1964, 2005],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([1050, 4617, 1299,  324, 1568, 2712, 2374, 1875,  830, 3431],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1531729.6250, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  5.927748042679114  minutes; Currently,  452  steps in.\n",
      "Losses Tokens:  3.39973384141922\n",
      "Router  Correctness:  0.2896466448903084\n",
      "Router Distribution:  6.681021666526794\n",
      "Router Distribution LOCAL:  7.010067939758301\n",
      "Index distribution:  tensor([1897, 1997, 2006, 1884, 2034, 2042, 2042, 2054, 1913, 2131],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([ 250, 1553, 1233,  375, 1796, 3484, 4020, 3107,  674, 3588],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1832096.3750, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  5.417786089451325  minutes; Currently,  528  steps in.\n",
      "Losses Tokens:  3.1968470454216003\n",
      "Router  Correctness:  0.2899195283651352\n",
      "Router Distribution:  6.6848972797393795\n",
      "Router Distribution LOCAL:  7.962515354156494\n",
      "Index distribution:  tensor([2050, 1832, 2048, 2086, 1924, 2033, 2068, 1942, 2080, 1937],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([4704, 1245, 1605, 2263, 1407, 2299, 2545, 1070, 2031,  911],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1089669.2500, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  4.916574527915503  minutes; Currently,  603  steps in.\n",
      "Losses Tokens:  3.0384655237197875\n",
      "Router  Correctness:  0.2905802190303802\n",
      "Router Distribution:  5.166453468799591\n",
      "Router Distribution LOCAL:  1.8746275901794434\n",
      "Index distribution:  tensor([2014, 2052, 1961, 1945, 1992, 2006, 2038, 2036, 1934, 2022],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([2780, 2265, 1425, 1781, 1808, 1706, 1631, 2509, 1628, 2547],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(201334.6094, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  4.407633893795323  minutes; Currently,  679  steps in.\n",
      "Losses Tokens:  2.973176419734955\n",
      "Router  Correctness:  0.2898838996887207\n",
      "Router Distribution:  5.480425703525543\n",
      "Router Distribution LOCAL:  4.72245454788208\n",
      "Index distribution:  tensor([1912, 2071, 2079, 1994, 1952, 2055, 1939, 2030, 1919, 2049],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([1408, 2766, 2213, 2415,  757, 3189,  919, 2579,  426, 3408],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1007732.6250, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  3.9072458569345803  minutes; Currently,  754  steps in.\n",
      "Losses Tokens:  2.8804189205169677\n",
      "Router  Correctness:  0.2847101092338562\n",
      "Router Distribution:  4.999813410639763\n",
      "Router Distribution LOCAL:  8.916616439819336\n",
      "Index distribution:  tensor([1895, 2028, 2101, 1966, 1950, 2025, 2128, 2074, 1987, 1846],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([1155, 2538, 3078, 1150,  640, 3020, 4057, 1454, 1683, 1305],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1089093.2500, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  3.4082064463232338  minutes; Currently,  829  steps in.\n",
      "Losses Tokens:  2.8448254585266115\n",
      "Router  Correctness:  0.2913759768009186\n",
      "Router Distribution:  4.040143269300461\n",
      "Router Distribution LOCAL:  9.03536319732666\n",
      "Index distribution:  tensor([1868, 2092, 2029, 1984, 2007, 2132, 1955, 1963, 2095, 1875],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([ 595, 2540, 1436, 1096, 1110, 2219,  870, 3391, 4728, 2095],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1490378.8750, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  2.906906925603352  minutes; Currently,  904  steps in.\n",
      "Losses Tokens:  2.77977968454361\n",
      "Router  Correctness:  0.28931387662887575\n",
      "Router Distribution:  5.3292904317379\n",
      "Router Distribution LOCAL:  3.4567506313323975\n",
      "Index distribution:  tensor([2005, 2075, 1957, 1952, 2114, 1956, 1955, 1990, 1995, 2001],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX:  tensor([3050, 3480,  700, 1095, 4750,  840, 1337,  612, 1543, 2673],\n",
      "       device='cuda:0')\n",
      "Index distribution MAX LOSS:  tensor(1773791.6250, device='cuda:0')\n",
      "\n",
      "\n",
      "\t\tTime left for TRAIN epoch:  2.4070569886859277  minutes; Currently,  979  steps in.\n"
     ]
    }
   ],
   "source": [
    "CE_LOSS = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "def test_model(optimiser, model, optimiser_router, router):\n",
    "    optimiser.eval()\n",
    "    model.eval()\n",
    "    optimiser_router.eval()\n",
    "    router.eval()\n",
    "\n",
    "    its = 0\n",
    "    start = time.time()\n",
    "    last = start\n",
    "    \n",
    "    losses_test = []\n",
    "    for batch in test_dataloader:\n",
    "        #pre-pad initial empty token\n",
    "        batch = torch.cat((torch.ones(batch.size()[0], 1).long() * PADDING_TOKEN, batch), dim=1)\n",
    "        batch = batch.to(DEVICE)\n",
    "        #for evaluation, we don't need to sample from the router; sampling is only a trick to train the router, so we just pick the most likely one!\n",
    "        indices = router(batch).argmax(dim=-1)\n",
    "        logits = model(batch, indices)\n",
    "        \n",
    "        target = batch[:,1:] #shifted left by one; we want to predict the next token\n",
    "        output = logits[:,:-1] #remove the last prediction; we don't want to predict anything at the last token\n",
    "        \n",
    "        loss = CE_LOSS(output.reshape(-1, output.size(-1)), target.reshape(-1)).mean()\n",
    "        losses_test.append(loss.item())\n",
    "        its += 1\n",
    "        if time.time() - last > 30: \n",
    "            print(\"\\t\\tTime left for TEST epoch: \", (time.time()-start)/its*(len(train_dataloader)-its), \" seconds.\")\n",
    "            last = time.time()\n",
    "    return losses_test\n",
    "\n",
    "def train_router(optimiser_router, routing, indices, loss_terms, losses_router_correctness, losses_router_distribution):\n",
    "    #train router:\n",
    "    optimiser_router.zero_grad()\n",
    "\n",
    "    bsize = routing.size()[0]\n",
    "    routing = routing.reshape(-1, NO_CLUSTERS)\n",
    "    indices = indices.reshape(-1)\n",
    "    probabilities_of_chosen = routing[torch.arange(indices.size()[0]), indices].view(bsize, -1)\n",
    "    \n",
    "    #FIRST : identify which losses are performing BELOW average,\n",
    "    #       i.e. error is HIGHER than average error (loss - avg is GREATER 0)\n",
    "    #       (ignore all losses that are performing better than average, i.e. are BELOW zero)\n",
    "    #       To further improve: Find the average loss PER POSITION; some positions are naturally more difficult than others, e.g. first vs last token in a story!\n",
    "    losses_below_average = (loss_terms.detach() - loss_terms.detach().mean(dim=0)[None]).clamp(min=0.0)\n",
    "    \n",
    "    #SECOND: punish those by punishing the probability of choosing this cluster; low probability samples don't matter as much here!\n",
    "    loss_router_correctness = ((losses_below_average * probabilities_of_chosen)).square().mean() * 10.0\n",
    "    \n",
    "    loss_router_distribution = 0.0\n",
    "    max_indices = routing.argmax(dim=-1)\n",
    "    for index in range(0, NO_CLUSTERS):\n",
    "        #get all indices where the model chose this cluster\n",
    "        indices_for_cluster = (indices == index).nonzero().squeeze()\n",
    "        \n",
    "        desired = max_indices.size()[0] / NO_CLUSTERS\n",
    "        actual = indices_for_cluster.size()[0]\n",
    "\n",
    "        edge_index = (desired - actual) / (max_indices.size()[0] / NO_CLUSTERS)\n",
    "\n",
    "        loss_router_distribution = loss_router_distribution + (routing[indices_for_cluster, index] - (routing[indices_for_cluster, index].detach() + edge_index).clamp(0.0, 1.0)).square().sum()\n",
    "    loss_router_distribution = loss_router_distribution / max_indices.size()[0] * 5000.0\n",
    "    \n",
    "    loss_router = loss_router_distribution + loss_router_correctness\n",
    "    \n",
    "    losses_router_distribution.append(loss_router_distribution.item())\n",
    "    losses_router_correctness.append(loss_router_correctness.item())\n",
    "\n",
    "    loss_router.backward()\n",
    "    optimiser_router.step()\n",
    "\n",
    "    return losses_router_correctness, losses_router_distribution\n",
    "\n",
    "def train_model(optimiser, model, optimiser_router, router, steps):\n",
    "    its = 0\n",
    "    start = time.time()\n",
    "    last = start\n",
    "    losses_train, losses_router_correctness, losses_router_distribution = [], [], []\n",
    "    \n",
    "    optimiser.train()\n",
    "    model.train()\n",
    "    optimiser_router.train()\n",
    "    router.train()\n",
    "    \n",
    "    if steps >= STEPS_ROUTER:\n",
    "        router.eval()\n",
    "        optimiser_router.eval()\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        steps += 1\n",
    "        #pre-pad initial empty token\n",
    "        batch = torch.cat((torch.ones(batch.size()[0], 1).long() * PADDING_TOKEN, batch), dim=1)\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        routing = F.softmax(router(batch), dim=-1) #get routing probabilities\n",
    "        routing_probabilities = routing + 0.2 / NO_CLUSTERS #add some epsilon noise to prevent 0 probabilities\n",
    "        routing_probabilities = routing_probabilities / routing_probabilities.sum(dim=-1, keepdim=True) #normalise routing probabilities\n",
    "\n",
    "        if steps < STEPS_ROUTER:\n",
    "            #during training, do SOFT PROBABILISTIC routing\n",
    "            #sample from routing probabilities\n",
    "            indices = torch.multinomial(routing_probabilities.reshape(routing_probabilities.size()[0] * routing_probabilities.size()[1], -1), 1).view(routing_probabilities.size()[0], routing_probabilities.size()[1])\n",
    "            max_indices = routing.argmax(dim=-1) #just to sanity check how we WOULD distribute\n",
    "        else:\n",
    "            #after router initialisation (first few thousand steps or so), do HARD routing: pick the best, we don't train it anymore\n",
    "            indices = routing.argmax(dim=-1)\n",
    "            max_indices = indices\n",
    "        \n",
    "        #remove last token from routing probabilities and routing --> we don't need to make a prediction after obtaining the last token\n",
    "        routing_probabilities = routing_probabilities[:,:-1,:]\n",
    "        routing = routing[:,:-1,:]\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        logits = model(batch, expert_labels=indices)\n",
    "        \n",
    "        indices = indices[:,:-1]\n",
    "        target = batch[:,1:] #shifted left by one; we want to predict the next token\n",
    "        output = logits[:,:-1] #remove the last prediction; we don't want to predict anything at the last token\n",
    "        \n",
    "        loss = CE_LOSS(output.reshape(-1, output.size(-1)), target.reshape(-1))\n",
    "        loss_terms = loss.clone().detach().view(output.size()[0], -1) #test if we need the clone even; we do need the detach\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimiser.step()\n",
    "        losses_train.append(loss.item())\n",
    "\n",
    "        #For training the router (only first few steps), we use two objectives:\n",
    "        #       a) distribute the samples evenly among the clusters\n",
    "        #       b) punish the router for choosing clusters that perform worse than average, i.e. get \"smart\" in choosing the right cluster\n",
    "        if steps < STEPS_ROUTER:\n",
    "            losses_router_correctness, losses_router_distribution = train_router(optimiser_router, routing, indices, loss_terms, losses_router_correctness, losses_router_distribution)\n",
    "        \n",
    "        its += 1\n",
    "        if time.time() - last > 30: \n",
    "            #print moving average over last 20 items:\n",
    "            print(\"Losses Tokens: \", sum(losses_train[-20:])/len(losses_train[-20:]))\n",
    "            if steps < STEPS_ROUTER:\n",
    "                #output loss terms for router to track it's progress\n",
    "                print(\"Router  Correctness: \", sum(losses_router_correctness[-20:])/len(losses_router_correctness[-20:]))\n",
    "                print(\"Router Distribution: \", sum(losses_router_distribution[-20:])/len(losses_router_distribution[-20:]))\n",
    "                print(\"Router Distribution LOCAL: \", losses_router_distribution[-1])\n",
    "            #output indices - how many of which ones? helps us to see if the router is doing his job\n",
    "            print(\"Index distribution: \", torch.unique(indices, return_counts=True)[1])\n",
    "            print(\"Index distribution MAX: \", torch.unique(max_indices, return_counts=True)[1])\n",
    "            print(\"Index distribution MAX LOSS: \", (torch.unique(max_indices, return_counts=True)[1]-2000.0).square().mean())\n",
    "            print(\"\\n\")\n",
    "            print(\"\\t\\tTime left for TRAIN epoch: \", (time.time()-start)/its*(len(train_dataloader)-its)/60, \" minutes; Currently, \",steps,\" steps in.\")\n",
    "            last = time.time()\n",
    "    return losses_train, steps\n",
    "\n",
    "def plot_store_losses(total_losses_train, total_losses_test, identifier, epoch):\n",
    "    #save losses:\n",
    "    torch.save(total_losses_train, \"stored/\"+identifier+\"_total_losses_train_\"+str(epoch)+\".pt\")\n",
    "    torch.save(total_losses_test, \"stored/\"+identifier+\"_total_losses_test_\"+str(epoch)+\".pt\")\n",
    "    \n",
    "    plt.plot(total_losses_train, label=\"train\")\n",
    "    plt.plot(total_losses_test, label=\"test\")\n",
    "    plt.title(\"Losses\")\n",
    "    plt.legend()\n",
    "    #save plot & show:\n",
    "    plt.savefig(\"stored/\"+identifier+\"_losses_\"+str(epoch)+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "def train():\n",
    "    identifier = \"sMoE\"\n",
    "\n",
    "    model = AutoregressiveDecoderTransformer(PADDING_TOKEN + 1, LARGEST_SEQUENCE_LENGTH+1, dim=512, num_layers=8, num_heads=8, dropout=DROPOUT_RATE).to(DEVICE)\n",
    "    print(\"Model has \", sum(p.numel() for p in model.parameters()), \" parameters.\")\n",
    "    #router model is about a quarter of the parameters, should suffice\n",
    "    router = AutoregressiveRouterTransformer(PADDING_TOKEN + 1, LARGEST_SEQUENCE_LENGTH + 1, dim=256, num_layers=8, num_heads=8, dropout=DROPOUT_RATE).to(DEVICE)\n",
    "    print(\"Router has \", sum(p.numel() for p in router.parameters()), \" parameters.\")\n",
    "\n",
    "    LR = 0.001 #works best for this model & dataset\n",
    "    SAMPLES_TO_GENERATE = 4\n",
    "\n",
    "    optimiser = schedulefree.AdamWScheduleFree(model.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=0.01, warmup_steps=STEPS_WARMUP) #also check 0.1; before: 0.01\n",
    "    optimiser_router = schedulefree.AdamWScheduleFree(router.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=0.01, warmup_steps=STEPS_WARMUP_ROUTER) #also check 0.1; before: 0.01\n",
    "\n",
    "    steps = 0 #how many steps we've done so far; used for switching off router training at some point\n",
    "\n",
    "    total_losses_train = []\n",
    "    total_losses_test  = []\n",
    "\n",
    "    for epoch in range(0, 100):\n",
    "        #1. Train:\n",
    "        losses_train, steps = train_model(optimiser, model, optimiser_router, router, steps)\n",
    "        #2. Evaluate:\n",
    "        with torch.no_grad():\n",
    "            losses_test = test_model(optimiser, model, optimiser_router, router)\n",
    "            \n",
    "            print(\"*** DONE WITH EPOCH \", epoch, \" - TRAIN LOSS: \", sum(losses_train)/len(losses_train), \" - TEST LOSS: \", sum(losses_test)/len(losses_test),\" ***\")\n",
    "            total_losses_train.append(sum(losses_train)/len(losses_train))\n",
    "            total_losses_test.append(sum(losses_test)/len(losses_test))\n",
    "\n",
    "            plot_store_losses(total_losses_train, total_losses_test, identifier, epoch)\n",
    "\n",
    "        #3. Inference / Generate:\n",
    "        with torch.no_grad():\n",
    "            #remember to always skip the first character befpre outputting, that's just the empty token (serving as the start of sequence token):\n",
    "            try:\n",
    "                sampled_max = model.generate_max((torch.ones(1, 1).long() * PADDING_TOKEN).to(DEVICE), 251, router)\n",
    "                sampled_mul = model.generate_mul((torch.ones(4, 1).long() * PADDING_TOKEN).to(DEVICE), 251, router)\n",
    "                sampled_nuc = model.generate_nuc((torch.ones(4, 1).long() * PADDING_TOKEN).to(DEVICE), 251, router)\n",
    "                print(\"\\tGENERATED SENTENCE  MAX: \")\n",
    "                print(\"\\t\\tStory 0: \",decode_BPE(sampled_max[0, 1:]))\n",
    "                print(\"\\tGENERATED SENTENCE  MUL: \")\n",
    "                for i in range(0, SAMPLES_TO_GENERATE):\n",
    "                    print(\"\\t\\tStory \"+str(i)+\": \",decode_BPE(sampled_mul[i, 1:]))\n",
    "                print(\"\\tGENERATED SENTENCE  NUC: \")\n",
    "                for i in range(0, SAMPLES_TO_GENERATE):\n",
    "                    print(\"\\t\\tStory \"+str(i)+\": \",decode_BPE(sampled_nuc[i, 1:]))\n",
    "                \n",
    "            except:\n",
    "                print(\"FAILED TO GENERATE\")\n",
    "                continue\n",
    "\n",
    "        #4. Store model & optimiser:\n",
    "        if True:\n",
    "            #(important for SF AdamW: only store stuff when in eval mode!)\n",
    "            #save model:\n",
    "            torch.save(model.state_dict(), \"stored/\"+identifier+\"_model_\"+str(epoch)+\".pt\")\n",
    "            #save optimiser:\n",
    "            torch.save(optimiser.state_dict(), \"stored/\"+identifier+\"_optimiser_\"+str(epoch)+\".pt\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
